#!/usr/bin/env python3
"""
Async client utilities for Together.ai with structured logging and streaming.

This module provides a robust, async-first client for interacting with the
Together AI API. It is designed for use in a multi-agent architecture, such as
within a FastAPI application context, and includes features like retry logic,
structured logging, and backward compatibility shims for a smooth migration.
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import time
from typing import AsyncGenerator, Optional

import httpx

# Configure module-level logger
logger = logging.getLogger(__name__)


class TogetherClient:
    """
    An async-native client for the Together AI API with support for streaming,
    retries, and structured logging.
    """

    BASE_URL = "https://api.together.xyz/v1/chat/completions"
    DEFAULT_MODEL = "mistralai/Mixtral-8x7B-Instruct-v0.1"

    def __init__(self, api_key: Optional[str] = None, max_retries: int = 3) -> None:
        """
        Initializes the client, loading the API key and setting up the
        HTTP client.

        Raises:
            RuntimeError: If the TOGETHER_API_KEY is not found in environment variables.
        """
        self.api_key = api_key or os.getenv("TOGETHER_API_KEY")
        if not self.api_key:
            raise RuntimeError(
                "TOGETHER_API_KEY not found. Please set it in your environment."
            )

        self.max_retries = max_retries
        self._client = httpx.AsyncClient(
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            },
            timeout=httpx.Timeout(60.0),
        )

    async def _retry_logic(
        self, attempt: int, error: Exception
    ) -> bool:
        """Determines if a retry is warranted and logs the attempt."""
        if attempt >= self.max_retries - 1:
            return False

        delay = 2 ** attempt
        logger.warning(
            "Request failed (attempt %d/%d): %s. Retrying in %ds...",
            attempt + 1,
            self.max_retries,
            error,
            delay,
        )
        await asyncio.sleep(delay)
        return True

    async def generate(
        self,
        prompt: str,
        model: str = DEFAULT_MODEL,
        **kwargs,
    ) -> str:
        """
        Generates a complete text response from a prompt.

        Args:
            prompt: The user prompt to send to the model.
            model: The model identifier to use for the completion.
            **kwargs: Additional parameters to pass to the API, e.g., `max_tokens`.

        Returns:
            The complete, generated text from the model.

        Raises:
            RuntimeError: If the request fails after all retry attempts.
        """
        start_time = time.monotonic()
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
            **kwargs,
        }

        for attempt in range(self.max_retries):
            try:
                response = await self._client.post(self.BASE_URL, json=payload)
                response.raise_for_status()
                data = response.json()
                text = data["choices"][0]["message"]["content"]
                logger.info(
                    "Generated response from %s in %.2fs",
                    model,
                    time.monotonic() - start_time,
                )
                return text
            except (httpx.RequestError, httpx.HTTPStatusError) as e:
                if not await self._retry_logic(attempt, e):
                    logger.error("Failed to generate response after %d retries.", self.max_retries)
                    raise RuntimeError("Failed to get a response from Together AI.") from e

        # This line should be unreachable, but acts as a safeguard.
        raise RuntimeError("Exhausted retries for generate request.")

    async def stream_generate(
        self,
        prompt: str,
        model: str = DEFAULT_MODEL,
        **kwargs,
    ) -> AsyncGenerator[str, None]:
        """
        Streams a response from the model, yielding tokens as they arrive.

        Args:
            prompt: The user prompt to send to the model.
            model: The model identifier to use for the completion.
            **kwargs: Additional parameters to pass to the API, e.g., `max_tokens`.

        Yields:
            Tokens (strings) as they are generated by the model.

        Raises:
            RuntimeError: If the request fails after all retry attempts.
        """
        start_time = time.monotonic()
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": True,
            **kwargs,
        }

        for attempt in range(self.max_retries):
            try:
                async with self._client.stream("POST", self.BASE_URL, json=payload) as resp:
                    resp.raise_for_status()
                    async for chunk in resp.aiter_lines():
                        if chunk.startswith("data:"):
                            data = chunk.split("data:", 1)[1].strip()
                            if data == "[DONE]":
                                break
                            try:
                                payload_json = json.loads(data)
                                delta = payload_json["choices"][0]["delta"].get("content")
                                if delta:
                                    yield delta
                            except (json.JSONDecodeError, KeyError):
                                logger.warning("Received malformed stream chunk: %s", data)
                    logger.info(
                        "Streamed response from %s in %.2fs",
                        model,
                        time.monotonic() - start_time,
                    )
                    return
            except (httpx.RequestError, httpx.HTTPStatusError) as e:
                if not await self._retry_logic(attempt, e):
                    logger.error("Failed to stream response after %d retries.", self.max_retries)
                    raise RuntimeError("Failed to get a streaming response from Together AI.") from e

        raise RuntimeError("Exhausted retries for stream_generate request.")

    async def close(self):
        """Gracefully closes the HTTP client."""
        await self._client.aclose()

def get_together_client() -> "TogetherClient":
    """Factory function to get an instance of the TogetherClient."""
    return TogetherClient()


def get_anthropic_client() -> "TogetherClient":
    """
    Backward-compatibility shim for dependent agents.

    Logs a warning and returns the new TogetherClient, ensuring that agents
    importing `get_anthropic_client` continue to work without modification.
    """
    logger.warning("get_anthropic_client() is deprecated; using TogetherClient instead.")
    return TogetherClient()


# --- Executable Smoke Test ---
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    async def _test():
        """Runs a quick smoke test of the client's methods."""
        print("--- Running Together AI Client Smoke Test ---")
        client = None
        try:
            client = TogetherClient()

            # 1. Test full generation
            print("\n1. Full response test:")
            full_response = await client.generate(
                "Hello from the ValueVerse platform!",
                max_tokens=20,
            )
            print(f"   -> Response: '{full_response.strip()}'")

            # 2. Test streaming generation
            print("\n2. Streaming test:")
            print("   -> Streaming: ", end="", flush=True)
            async for token in client.stream_generate(
                "Write a short haiku about asynchronous code.",
                max_tokens=20,
            ):
                print(token, end="", flush=True)
            print("\n--- Test Complete ---")

        except RuntimeError as e:
            print(f"\n--- A critical error occurred: {e} ---")
            print("Please ensure your TOGETHER_API_KEY is set correctly.")
        finally:
            if client and client._client:
                await client._client.aclose()

    asyncio.run(_test())
